# HardSample

## Project Overview
HardSample is a difficult sample repository designed to enhance the reasoning capability of large models. We have found through collection and analysis that some large models, even when using chain-of-thought (COT) prompts, still fail to answer questions correctly. r1's reinforcement learning addresses this issue by rewarding the model's answers based on rules, thereby enabling the model to generate chain-of-thought processes on its own.

A paper from ByteDance has already utilized this method, and there are still many explorations of reasoning models in the industry. Why hasn't something as impressive as r1 emerged?

Recent reproduction experiments have captured my attention; they are mostly built on carefully designed tasks and high-quality datasets, achieving unprecedented results with minimal learning steps.

This is crucial.

If these specially designed tasks and their datasets were expanded by 100 times, 1000 times, or even 10000 times, could we obtain more powerful models and establish higher-quality reasoning model data engineering?

I believe that designing high-quality reasoning tasks and providing high-quality samples is the key to effectively improving the reasoning level of large models. In other words, large models inherently have the potential for deep thinking, and what we need to do is to unlock it.

## How will we do it?

Drawing on the experience from the s1 work, we will use two models to mine difficult samples from the reasoning task dataset.

The specific process is as follows:

1. Select a recently designed task or dataset aimed at enhancing the reasoning ability of large models.
2. Use the same prompt as tinyzero to let qwen-2.5-7B-instruct answer questions in this dataset.
3. Send the samples that were answered incorrectly in the first round to qwen-max for a second round of answering.

Through these steps, we can obtain:

- Samples answered incorrectly by qwen-2.5-7B-instruct, which are extremely suitable for training as they benefit from the rule-based reinforcement learning rewards transitioning from incorrect to correct answers.
- Samples without answers generated by qwen-2.5-7B-instruct, often indicating incorrect answer formats or severe hesitation from the model, making them very difficult samples suitable for continuous reinforcement learning.
- Samples answered incorrectly by qwen-max.
- Samples without answers generated by qwen-max.

## Included Datasets
- **S1 Dataset**: As a rare high-quality dataset, we have already included the S1 dataset in the project.

## Update Plan
- **[ ] K-and-K/knights-and-knaves**: This task focuses on identifying robbers and knights, requiring high levels of dialectical ability. Recently, there have been successful attempts, and we plan to incorporate it into the current plan. It is expected to be online soon.
- **[ ] countdown**: This task from tinyzero encourages the model to attempt and verify, generating significant influence within the community with large data volume. We will mine the difficult parts of this dataset.
- **[ ] homebrewltd/Maze-Reasoning-GRPO-v0.1**: This interesting project models a 2D maze graph using text and lets the model navigate the maze using text. It appears to be a challenging task that can allow the model to backtrack and perceive modeling, and we plan to include it in our mining plan.

## Notes
This project focuses on enhancing the reasoning capability of large models, so task design and sample selection will strictly adhere to relevant principles. If the dataset authors or any stakeholders need to communicate, please feel free to contact me.

## Contribution
We welcome any developers or researchers to join us in building and optimizing the HardSample difficult sample repository. Your contributions will significantly impact the improvement of large models' reasoning capabilities.

## Contact Us
If you have any questions or suggestions, please contact us via the following channels:
- Email: 3306065226@qq.com
- GitHub: https://github.com/jiangchengchengark
